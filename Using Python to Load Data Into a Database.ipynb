{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f32a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Using Python to load data into a database offers several advantages:\n",
    "\n",
    "#  Automation: Python provides powerful libraries and tools for data manipulation and processing. By writing Python \n",
    "#  scripts or applications, you can automate the process of loading data into a database, reducing manual effort and \n",
    "#  potential errors.\n",
    "\n",
    "#  Flexibility: Python's versatility allows you to work with various data formats, including CSV, JSON, Excel, and more. \n",
    "#  You can easily read data from different sources, transform it as needed, and load it into the database.\n",
    "\n",
    "#  Integration: Python integrates seamlessly with popular database management systems (DBMS) through libraries like \n",
    "#  SQLAlchemy, pyodbc, and psycopg2. This allows you to connect to different types of databases (e.g., SQL, NoSQL) and \n",
    "#  interact with them programmatically.\n",
    "\n",
    "#  Data Transformation: Python's extensive ecosystem of libraries (e.g., pandas, NumPy) provides powerful tools for data \n",
    "#  manipulation and transformation. You can clean, reshape, and preprocess data before inserting it into the database, \n",
    "#  ensuring data quality and consistency.\n",
    "\n",
    "#  Customization: Python allows you to tailor the data loading process to your specific requirements. You can implement \n",
    "#  custom logic, error handling, and data validation as part of the loading process, ensuring that data is loaded \n",
    "#  correctly and efficiently.\n",
    "\n",
    "#  Scalability: Python's scalability makes it suitable for handling large volumes of data. You can optimize the loading \n",
    "#  process to handle big data scenarios, parallelize tasks, and optimize performance for faster data ingestion.\n",
    "\n",
    "#  Cross-Platform Compatibility: Python is platform-independent, meaning your data loading scripts can run on different \n",
    "#  operating systems (e.g., Windows, macOS, Linux) without modification. This makes it easy to deploy and manage data \n",
    "#  loading processes across different environments.\n",
    "\n",
    "#  Open Source Ecosystem: Python's open-source nature means you have access to a vast ecosystem of libraries, frameworks, \n",
    "#  and community contributions. You can leverage existing solutions and best practices to streamline the data loading \n",
    "#  process and accelerate development.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "063525cf-8a3f-4890-bb97-a262553ef352",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data from CSV file on PC, to a dynamically created new table in SQL Server\n",
    "# pip install pyodbc\n",
    "import csv\n",
    "import pyodbc\n",
    "\n",
    "# Establish connection to SQL Server with Username and Password\n",
    "# conn_str = (\n",
    "#     r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "#     r'SERVER=MyDevice\\SQLEXPRESS01;'\n",
    "#     r'DATABASE=my_test;'\n",
    "#     r'UID=your_username;'\n",
    "#     r'PWD=your_password;'\n",
    "# )\n",
    "\n",
    "\n",
    "# Establish connection to SQL Server using Windows authentication\n",
    "conn_str = (\n",
    "    r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "    r'SERVER=MyDevice\\SQLEXPRESS01;'\n",
    "    r'DATABASE=my_test;'\n",
    "    r'Trusted_Connection=yes;'\n",
    ")\n",
    "conn = pyodbc.connect(conn_str)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Read CSV file to get column names and data\n",
    "csv_file = r'C:\\Users\\ryan_\\Desktop\\Book1.csv'\n",
    "with open(csv_file, 'r', newline='') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    columns = next(csvreader)  # Get column names from header row\n",
    "\n",
    "# Create SQL CREATE TABLE statement dynamically\n",
    "create_table_query = f\"CREATE TABLE NewTable ({', '.join(f'{col} VARCHAR(MAX)' for col in columns)})\"\n",
    "\n",
    "# Execute CREATE TABLE query\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# Read CSV data again to insert into the new table\n",
    "with open(csv_file, 'r', newline='') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    next(csvreader)  # Skip header row\n",
    "    for row in csvreader:\n",
    "        # Generate INSERT INTO query dynamically\n",
    "        insert_query = f\"INSERT INTO NewTable VALUES ({', '.join('?' * len(row))})\"\n",
    "        \n",
    "        # Execute INSERT INTO query\n",
    "        cursor.execute(insert_query, row)\n",
    "        conn.commit()\n",
    "\n",
    "# Close cursor and connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "092fc832",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data from CSV file on PC, to dataframe, and then push dataframe to a dynamically created new table in SQL Server\n",
    "# pip install pandas sqlalchemy\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Read CSV file into DataFrame\n",
    "df = pd.read_csv(r'C:\\\\Users\\\\ryan_\\\\Book1.csv')\n",
    "\n",
    "# Create connection to SQL Server\n",
    "# In this case:\n",
    "# My Server Name = 'MyDevice\\SQLEXPRESS01'\n",
    "# My Database Name = 'my_test'    \n",
    "engine = create_engine('mssql+pyodbc://MyDevice\\SQLEXPRESS01/my_test?driver=ODBC+Driver+17+for+SQL+Server')\n",
    "\n",
    "# Insert data into SQL Server table\n",
    "df.to_sql('MyTable', con=engine, if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe018221",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data from JSON file on PC, a dynamically created new table in MongoDB\n",
    "import pymongo\n",
    "import json\n",
    "from pymongo import MongoClient, InsertOne\n",
    "\n",
    "client = pymongo.MongoClient('mongodb://localhost:27017')\n",
    "db = client.local\n",
    "collection = db.NewTable\n",
    "requesting = []\n",
    "\n",
    "with open(r\"C:\\\\Users\\\\ryan_\\\\odds_todaysGames.json\") as f:\n",
    "    for jsonObj in f:\n",
    "        myDict = json.loads(jsonObj)\n",
    "        requesting.append(InsertOne(myDict))\n",
    "\n",
    "result = collection.bulk_write(requesting)\n",
    "client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82b155cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data from JSON file on PC, to dataframe, and then push dataframe to a dynamically created new table in MongoDB\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Load data from JSON file into a DataFrame\n",
    "df = pd.read_json(r\"C:\\\\Users\\\\ryan_\\\\odds_todaysGames.json\")\n",
    "\n",
    "# Convert DataFrame to list of dictionaries\n",
    "data = df.to_dict(orient='records')\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.local\n",
    "collection = db.TableJson\n",
    "\n",
    "# Insert documents into MongoDB\n",
    "collection.insert_many(data)\n",
    "\n",
    "# Close MongoDB connection\n",
    "client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c49766a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SQL Server and MongoDB are both popular database management systems, but they differ in several key aspects. Here are \n",
    "# some major similarities and differences between the two:\n",
    "\n",
    "    \n",
    "\n",
    "# Similarities:\n",
    "#  Data Storage: Both SQL Server and MongoDB are used to store and manage data.\n",
    "#  Indexes: Both support indexing to improve query performance.\n",
    "#  Security: They offer security features such as user authentication, authorization, and encryption to protect data.\n",
    "#  Backup and Recovery: Both provide mechanisms for backup and recovery of data to ensure data integrity and availability.\n",
    "#  Scalability: Both SQL Server and MongoDB support scaling horizontally and vertically to handle increasing data volumes \n",
    "#  and user loads.\n",
    "\n",
    "\n",
    "\n",
    "# Differences:\n",
    "\n",
    "#  Data Model:\n",
    "#  SQL Server: Follows a relational data model and stores data in tables with rows and columns. It uses SQL \n",
    "#  (Structured Query Language) for querying data using predefined schemas.\n",
    "#  MongoDB: Follows a document-oriented data model and stores data in flexible, JSON-like documents. It does \n",
    "#  not require a predefined schema, allowing for dynamic and nested data structures. Queries are made using \n",
    "#  MongoDB's query language and operators.\n",
    "\n",
    "#  Schema:\n",
    "#  SQL Server: Requires a predefined schema where the structure of the data (tables, columns, data types, constraints) \n",
    "#  must be defined before data insertion.\n",
    "#  MongoDB: Allows flexible schema design where documents within a collection can have varying structures. It supports \n",
    "#  dynamic schema evolution, enabling easy modification of data structures without downtime.\n",
    "\n",
    "#  Query Language:\n",
    "#  SQL Server: Uses SQL (Structured Query Language) for querying data. SQL provides a standardized syntax for interacting \n",
    "#  with relational databases.\n",
    "#  MongoDB: Uses a query language similar to JavaScript and provides a rich set of operators for querying nested and \n",
    "#  complex data structures within documents.\n",
    "\n",
    "#  Transactions:\n",
    "#  SQL Server: Supports ACID (Atomicity, Consistency, Isolation, Durability) transactions for ensuring data integrity \n",
    "#  in multi-operation transactions.\n",
    "#  MongoDB: Supports atomic operations on a single document but does not support multi-document transactions across \n",
    "#  multiple collections/documents in a single transaction.\n",
    "\n",
    "#  Scaling:\n",
    "#  SQL Server: Typically scales vertically by adding more resources (CPU, memory, storage) to a single server.\n",
    "#  MongoDB: Scales horizontally by distributing data across multiple servers (sharding), allowing for linear scalability \n",
    "#  as data volume increases.\n",
    "\n",
    "#  Community and Ecosystem:\n",
    "#  SQL Server: Developed and maintained by Microsoft, with strong community support and a wide range of third-party tools \n",
    "#  and integrations.\n",
    "#  MongoDB: Developed and maintained by MongoDB Inc., with an active open-source community and a rich ecosystem of \n",
    "#  libraries, frameworks, and tools.\n",
    "\n",
    "#  Use Cases:\n",
    "#  SQL Server: Well-suited for applications with structured and relational data, such as transactional systems, enterprise \n",
    "#  applications, and data warehouses.\n",
    "#  MongoDB: Ideal for applications with unstructured or semi-structured data, real-time analytics, content management systems, \n",
    "#  and use cases requiring flexibility and scalability.\n",
    "\n",
    "# Overall, the choice between SQL Server and MongoDB depends on factors such as the nature of your data, the complexity of \n",
    "#  your application, scalability requirements, and development preferences.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
