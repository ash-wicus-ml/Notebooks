{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 336776 entries, 0 to 336775\n",
      "Data columns (total 19 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   year            336776 non-null  int64  \n",
      " 1   month           336776 non-null  int64  \n",
      " 2   day             336776 non-null  int64  \n",
      " 3   dep_time        328521 non-null  float64\n",
      " 4   sched_dep_time  336776 non-null  int64  \n",
      " 5   dep_delay       328521 non-null  float64\n",
      " 6   arr_time        328063 non-null  float64\n",
      " 7   sched_arr_time  336776 non-null  int64  \n",
      " 8   arr_delay       327346 non-null  float64\n",
      " 9   carrier         336776 non-null  object \n",
      " 10  flight          336776 non-null  int64  \n",
      " 11  tailnum         334264 non-null  object \n",
      " 12  origin          336776 non-null  object \n",
      " 13  dest            336776 non-null  object \n",
      " 14  air_time        327346 non-null  float64\n",
      " 15  distance        336776 non-null  int64  \n",
      " 16  hour            336776 non-null  int64  \n",
      " 17  minute          336776 non-null  int64  \n",
      " 18  time_hour       336776 non-null  object \n",
      "dtypes: float64(5), int64(9), object(5)\n",
      "memory usage: 48.8+ MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Transformer models have revolutionized the field of natural language processing (NLP) and are the backbone of many state-of-the-art models \n",
    "# like BERT, GPT, and T5.\n",
    "\n",
    "# The transformer architecture consists of an encoder and a decoder, each built from layers of self-attention and feed-forward neural networks.\n",
    "\n",
    "# The Encoder consists of:\n",
    "\n",
    "# Input Embeddings: The input text is converted into embeddings, which are vectors representing the words.\n",
    "\n",
    "# Positional Encoding: Since transformers do not have a sequence-awareness like RNNs, positional encodings are added to input embeddings \n",
    "# to give the model information about the order of words.\n",
    "\n",
    "# Multi-Head Self-Attention: Each word attends to every other word in the sentence, allowing the model to capture context from all positions. \n",
    "# Multiple attention heads allow the model to focus on different parts of the sentence simultaneously.\n",
    "\n",
    "# Feed-Forward Neural Network: After attention, the data is passed through a feed-forward neural network.\n",
    "\n",
    "# Layer Normalization and Residual Connections: These help in stabilizing and speeding up the training process.\n",
    "\n",
    "# The Decoder is similar but slightly different:\n",
    "# Similar to the encoder but with an additional attention layer that helps the decoder focus on relevant parts of the input sentence.\n",
    "        \n",
    "# These models are pre-trained, which is very convenient because it saves us a ton of time, as we don't have to train and tune the models ourselves.\n",
    "\n",
    "# Methods:\n",
    "# Masked Language Modeling (MLM): Used in BERT. Randomly masks some of the words in the input and the model is trained to predict them.\n",
    "# Next Sentence Prediction (NSP): Also used in BERT. The model predicts whether a given sentence follows another sentence in the original text.\n",
    "# Causal Language Modeling (CLM): Used in GPT. Predicts the next word in a sequence, training the model to generate coherent text.\n",
    "\n",
    "# Fine-tuning\n",
    "# After pre-training, the model can be fine-tuned on specific tasks like question answering, text classification, or translation using \n",
    "# task-specific data.\n",
    "\n",
    "# So, how does this actually work?\n",
    "# Self-Attention: The core idea is to compute a weighted representation of the input sequence where each word pays attention to all other words. \n",
    "# This is achieved using:\n",
    "#  Query (Q): Represents the word for which attention is being calculated.\n",
    "# Key (K): Represents the words being attended to.\n",
    "# Value (V): The values corresponding to the keys, which contribute to the output representation.\n",
    "# The attention score is calculated using the dot product of Q and K, scaled, and passed through a softmax to get the attention weights.\n",
    "\n",
    "# Multi-Head Attention\n",
    "# Multiple attention mechanisms (heads) are used in parallel to capture different aspects of the relationships between words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ryan_\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ryan_\\anaconda3\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ryan_\\anaconda3\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Processing PDFs: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:42<00:00,  8.47s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# I did a Google search for this: 'federal reserve pdf'\n",
    "# I Downloaded the first 5 PDF files that I saw and put them in a folder on my desktop.\n",
    "\n",
    "# The script below uses a question-answering model initialized from the Hugging Face transformers library, specifically the \n",
    "# bert-large-uncased-whole-word-masking-finetuned-squad model.\n",
    "\n",
    "import os\n",
    "import pdfplumber\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Function to extract text from a single PDF using pdfplumber\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = ''\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text\n",
    "    return text\n",
    "\n",
    "# Function to process all PDFs with a progress bar\n",
    "def get_all_texts(folder_path):\n",
    "    pdf_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "    texts = []\n",
    "    for pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "        texts.append(extract_text_from_pdf(pdf_file))\n",
    "    return ' '.join(texts)\n",
    "\n",
    "# Initialize the question-answering pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "model = AutoModelForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "qa_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
    "\n",
    "def ask_question(context, question):\n",
    "    return qa_pipeline({'context': context, 'question': question})\n",
    "\n",
    "# Specify the folder containing the files\n",
    "folder_path = 'C:\\\\Users\\\\ryan_\\\\Desktop\\\\All_Docs\\\\'\n",
    "context = get_all_texts(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federal Reserve Board\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ask a question\n",
    "question = \"What are these documents about?\"\n",
    "answer = ask_question(context, question)\n",
    "print(answer['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervising and examining state member banks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ask a question\n",
    "question = \"What is the main purpose of the Federal Reserve Bank?\"\n",
    "answer = ask_question(context, question)\n",
    "print(answer['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef212eb5c00d48fda66b4a93ad969878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/451 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99baf2536124408befb9b62a02355bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc953eb5542482da0177d667aa047ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93df0204e11842dcbd69a3166c4518d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29883333f665419e8ef21a7e2927c617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.87it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This script below also uses a question-answering model initialized from the transformers library but with a different model: \n",
    "# distilbert-base-uncased-distilled-squad.\n",
    "\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "\n",
    "# Function to extract text from a single PDF using PyMuPDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = ''\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "# Function to process all PDFs with a progress bar\n",
    "def get_all_texts(folder_path):\n",
    "    pdf_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "    texts = []\n",
    "    for pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "        texts.append(extract_text_from_pdf(pdf_file))\n",
    "    return ' '.join(texts)\n",
    "\n",
    "# Initialize the question-answering pipeline\n",
    "qa_pipeline = pipeline('question-answering', model='distilbert-base-uncased-distilled-squad')\n",
    "\n",
    "def ask_question(context, question):\n",
    "    result = qa_pipeline({'context': context, 'question': question})\n",
    "    return result['answer']\n",
    "\n",
    "# Specify the folder containing the files\n",
    "folder_path = 'C:\\\\Users\\\\ryan_\\\\Desktop\\\\All_Docs\\\\'\n",
    "context = get_all_texts(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 1913\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ask a question\n",
    "question = \"In what year was the Federal Reserve system created?\"\n",
    "answer = ask_question(context, question)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: state-chartered banks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ask a question\n",
    "question = \"Who are the memebers of the Federal Reserve system?\"\n",
    "answer = ask_question(context, question)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Popular Transformer Models include the following\n",
    "# BERT (Bidirectional Encoder Representations from Transformers)\n",
    "# Training: Trained using MLM and NSP.\n",
    "# Usage: Good for tasks requiring understanding of context from both directions, such as question answering and text classification.\n",
    "\n",
    "# GPT (Generative Pre-trained Transformer)\n",
    "# Training: Uses CLM, training to predict the next word in a sequence.\n",
    "# Usage: Excellent for text generation tasks.\n",
    "\n",
    "# T5 (Text-to-Text Transfer Transformer)\n",
    "# Training: Converts all tasks into a text-to-text format, where both input and output are text strings.\n",
    "# Usage: Versatile, used for translation, summarization, and more.\n",
    "\n",
    "# Applications of Transformer Models\n",
    "# Question Answering: Understanding context and extracting relevant information.\n",
    "# Text Generation: Generating coherent and contextually relevant text.\n",
    "# Translation: Translating text from one language to another.\n",
    "# Summarization: Condensing long texts into shorter summaries.\n",
    "# Sentiment Analysis: Determining the sentiment expressed in a text.\n",
    "\n",
    "# Understanding their architecture and training helps in leveraging these models effectively for various NLP tasks. By using pre-trained \n",
    "# models like BERT and GPT, one can achieve high performance on complex language tasks with relatively little fine-tuning on specific datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# END!!!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
