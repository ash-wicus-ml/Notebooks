{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f32a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Using Python to load data into a database offers several advantages:\n",
    "\n",
    "#  Automation: Python provides powerful libraries and tools for data manipulation and processing. By writing Python \n",
    "#  scripts or applications, you can automate the process of loading data into a database, reducing manual effort and \n",
    "#  potential errors.\n",
    "\n",
    "#  Flexibility: Python's versatility allows you to work with various data formats, including CSV, JSON, Excel, and more. \n",
    "#  You can easily read data from different sources, transform it as needed, and load it into the database.\n",
    "\n",
    "#  Integration: Python integrates seamlessly with popular database management systems (DBMS) through libraries like \n",
    "#  SQLAlchemy, pyodbc, and psycopg2. This allows you to connect to different types of databases (e.g., SQL, NoSQL) and \n",
    "#  interact with them programmatically.\n",
    "\n",
    "#  Data Transformation: Python's extensive ecosystem of libraries (e.g., pandas, NumPy) provides powerful tools for data \n",
    "#  manipulation and transformation. You can clean, reshape, and preprocess data before inserting it into the database, \n",
    "#  ensuring data quality and consistency.\n",
    "\n",
    "#  Customization: Python allows you to tailor the data loading process to your specific requirements. You can implement \n",
    "#  custom logic, error handling, and data validation as part of the loading process, ensuring that data is loaded \n",
    "#  correctly and efficiently.\n",
    "\n",
    "#  Scalability: Python's scalability makes it suitable for handling large volumes of data. You can optimize the loading \n",
    "#  process to handle big data scenarios, parallelize tasks, and optimize performance for faster data ingestion.\n",
    "\n",
    "#  Cross-Platform Compatibility: Python is platform-independent, meaning your data loading scripts can run on different \n",
    "#  operating systems (e.g., Windows, macOS, Linux) without modification. This makes it easy to deploy and manage data \n",
    "#  loading processes across different environments.\n",
    "\n",
    "#  Open Source Ecosystem: Python's open-source nature means you have access to a vast ecosystem of libraries, frameworks, \n",
    "#  and community contributions. You can leverage existing solutions and best practices to streamline the data loading \n",
    "#  process and accelerate development.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "063525cf-8a3f-4890-bb97-a262553ef352",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data from CSV file on PC, to a dynamically created new table in SQL Server\n",
    "# pip install pyodbc\n",
    "import csv\n",
    "import pyodbc\n",
    "\n",
    "# Establish connection to SQL Server with Username and Password\n",
    "# conn_str = (\n",
    "#     r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "#     r'SERVER=MyDevice\\SQLEXPRESS01;'\n",
    "#     r'DATABASE=my_test;'\n",
    "#     r'UID=your_username;'\n",
    "#     r'PWD=your_password;'\n",
    "# )\n",
    "\n",
    "\n",
    "# Establish connection to SQL Server using Windows authentication\n",
    "conn_str = (\n",
    "    r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "    r'SERVER=MyDevice\\SQLEXPRESS01;'\n",
    "    r'DATABASE=my_test;'\n",
    "    r'Trusted_Connection=yes;'\n",
    ")\n",
    "conn = pyodbc.connect(conn_str)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Read CSV file to get column names and data\n",
    "csv_file = r'C:\\Users\\ryan_\\Desktop\\Book1.csv'\n",
    "with open(csv_file, 'r', newline='') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    columns = next(csvreader)  # Get column names from header row\n",
    "\n",
    "# Create SQL CREATE TABLE statement dynamically\n",
    "create_table_query = f\"CREATE TABLE NewTable ({', '.join(f'{col} VARCHAR(MAX)' for col in columns)})\"\n",
    "\n",
    "# Execute CREATE TABLE query\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# Read CSV data again to insert into the new table\n",
    "with open(csv_file, 'r', newline='') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    next(csvreader)  # Skip header row\n",
    "    for row in csvreader:\n",
    "        # Generate INSERT INTO query dynamically\n",
    "        insert_query = f\"INSERT INTO NewTable VALUES ({', '.join('?' * len(row))})\"\n",
    "        \n",
    "        # Execute INSERT INTO query\n",
    "        cursor.execute(insert_query, row)\n",
    "        conn.commit()\n",
    "\n",
    "# Close cursor and connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "092fc832",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data from CSV file on PC, to dataframe, and then push dataframe to a dynamically created new table in SQL Server\n",
    "# pip install pandas sqlalchemy\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Read CSV file into DataFrame\n",
    "df = pd.read_csv(r'C:\\\\Users\\\\ryan_\\\\Book1.csv')\n",
    "\n",
    "# Create connection to SQL Server\n",
    "# In this case:\n",
    "# My Server Name = 'MyDevice\\SQLEXPRESS01'\n",
    "# My Database Name = 'my_test'    \n",
    "engine = create_engine('mssql+pyodbc://MyDevice\\SQLEXPRESS01/my_test?driver=ODBC+Driver+17+for+SQL+Server')\n",
    "\n",
    "# Insert data into SQL Server table\n",
    "df.to_sql('MyTable', con=engine, if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe018221",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data from JSON file on PC, a dynamically created new table in MongoDB\n",
    "import pymongo\n",
    "import json\n",
    "from pymongo import MongoClient, InsertOne\n",
    "\n",
    "client = pymongo.MongoClient('mongodb://localhost:27017')\n",
    "db = client.local\n",
    "collection = db.NewTable\n",
    "requesting = []\n",
    "\n",
    "with open(r\"C:\\\\Users\\\\ryan_\\\\odds_todaysGames.json\") as f:\n",
    "    for jsonObj in f:\n",
    "        myDict = json.loads(jsonObj)\n",
    "        requesting.append(InsertOne(myDict))\n",
    "\n",
    "result = collection.bulk_write(requesting)\n",
    "client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82b155cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data from JSON file on PC, to dataframe, and then push dataframe to a dynamically created new table in MongoDB\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Load data from JSON file into a DataFrame\n",
    "df = pd.read_json(r\"C:\\\\Users\\\\ryan_\\\\odds_todaysGames.json\")\n",
    "\n",
    "# Convert DataFrame to list of dictionaries\n",
    "data = df.to_dict(orient='records')\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.local\n",
    "collection = db.TableJson\n",
    "\n",
    "# Insert documents into MongoDB\n",
    "collection.insert_many(data)\n",
    "\n",
    "# Close MongoDB connection\n",
    "client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7167c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# JSON (JavaScript Object Notation) and XML (eXtensible Markup Language) are both popular formats for representing \n",
    "#  structured data. Here's a comparison between the two:\n",
    "\n",
    "#  Syntax:\n",
    "#  JSON uses a lightweight syntax that resembles JavaScript object notation. It consists of key-value pairs and supports arrays.\n",
    "#  XML uses tags to define elements, attributes, and hierarchies. It's more verbose compared to JSON.\n",
    "\n",
    "#  Readability:\n",
    "#  JSON tends to be more readable and compact, especially for simple data structures.\n",
    "#  XML can be more verbose and less human-readable due to its tag-based structure.\n",
    "\n",
    "#  Data Types:\n",
    "#  JSON supports basic data types such as strings, numbers, booleans, arrays, and objects. It does not have built-in \n",
    "#  support for dates or binary data.\n",
    "#  XML is text-based and can represent complex data structures, but it does not have built-in support for specific data types.\n",
    "\n",
    "#  Usage:\n",
    "#  JSON is commonly used for web APIs, configuration files, and data interchange between systems due to its simplicity and \n",
    "#  ease of use.\n",
    "#  XML is widely used in various domains, including web services (SOAP), document storage (e.g., XHTML), and configuration files.\n",
    "\n",
    "#  Parsing:\n",
    "#  JSON parsing is usually faster and more efficient compared to XML parsing, especially in JavaScript environments.\n",
    "#  XML parsing can be more complex and may require more processing overhead, particularly for large documents.\n",
    "\n",
    "#  Schema Support:\n",
    "#  JSON does not have built-in schema support, although JSON Schema exists as a separate specification for defining JSON \n",
    "#  data structure constraints.\n",
    "#  XML has built-in support for Document Type Definitions (DTD) and XML Schema Definition (XSD), which allow for stricter \n",
    "#  validation of document structure.\n",
    "\n",
    "#  Tooling:\n",
    "#  JSON is well-supported by modern programming languages and has extensive tooling available for parsing, validation, \n",
    "#  and manipulation.\n",
    "#  XML has been around longer and has a rich ecosystem of tools and libraries for processing and transforming XML documents.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac2cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Based on what we saw above, we can use a similar approach to insert an XML file into MongoDB. However, since XML is \n",
    "# more hierarchical in nature compared to JSON, you'll need to parse the XML file and map its structure to a MongoDB \n",
    "# document format.\n",
    "\n",
    "# Parse the XML file and convert it into a Python dictionary, or list of dictionaries, representing MongoDB documents.\n",
    "\n",
    "# Here's a general approach to insert XML data into MongoDB...\n",
    "# Use the InsertOne or insert_many method to insert the documents into MongoDB.\n",
    "    \n",
    "import xml.etree.ElementTree as ET\n",
    "import pymongo\n",
    "from pymongo import MongoClient, InsertOne\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = pymongo.MongoClient('mongodb://localhost:27017')\n",
    "db = client.local\n",
    "collection = db.NewTable\n",
    "requesting = []\n",
    "\n",
    "# Parse the XML file\n",
    "tree = ET.parse(r'C:\\Users\\ryan_\\test.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# Convert XML to MongoDB documents\n",
    "def xml_to_documents(element):\n",
    "    doc = {}\n",
    "    for child in element:\n",
    "        if child.tag == 'change':\n",
    "            doc[child.find('change_description').text] = child.find('new_value').text\n",
    "        else:\n",
    "            doc[child.tag] = child.attrib\n",
    "            doc.update(xml_to_documents(child))\n",
    "    return doc\n",
    "\n",
    "# Iterate through top-level elements\n",
    "for child in root:\n",
    "    doc = xml_to_documents(child)\n",
    "    requesting.append(InsertOne(doc))\n",
    "\n",
    "# Perform bulk insert into MongoDB\n",
    "result = collection.bulk_write(requesting)\n",
    "\n",
    "# Close MongoDB connection\n",
    "client.close()\n",
    "print(result.inserted_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# HEre is how we can read the same XML file into a structured dataframe, and then push dataframe to a dynamically created new \n",
    "# table in MongoDB\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Step 1: Parse XML file and extract data into a DataFrame\n",
    "tree = ET.parse(r'C:\\Users\\ryan_\\test.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "data = []\n",
    "for horse in root.findall('.//start_changes/horse'):\n",
    "    horse_data = {'horse_name': horse.get('horse_name'), 'program_number': horse.get('program_number')}\n",
    "    for change in horse.findall('change'):\n",
    "        change_description = change.find('change_description').text\n",
    "        new_value = change.find('new_value').text\n",
    "        horse_data[change_description] = new_value\n",
    "    data.append(horse_data)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df.head)\n",
    "\n",
    "# Step 2: Connect to MongoDB and create a new collection dynamically\n",
    "client = pymongo.MongoClient('mongodb://localhost:27017')\n",
    "db = client.local\n",
    "collection_name = 'NewCollection'  # Change this to your desired collection name\n",
    "collection = db[collection_name]\n",
    "\n",
    "# Step 3: Insert DataFrame records into MongoDB collection\n",
    "records = df.to_dict(orient='records')\n",
    "collection.insert_many(records)\n",
    "\n",
    "# Close MongoDB connection\n",
    "client.close()\n",
    "print(result.inserted_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae3a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Source for XML:\n",
    "# urlfile = \"http://www.equibase.com/premium/eqbLateChangeXMLDownload.cfm\"\n",
    "\n",
    "# Download XML file using Python\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "urlfile = \"http://www.equibase.com/premium/eqbLateChangeXMLDownload.cfm\"\n",
    "\n",
    "request = urllib.request.Request(urlfile, headers={'User-Agent': 'Mozilla'})\n",
    "response = urllib.request.urlopen(request)\n",
    "with open(\"C:\\\\Users\\\\ryan_\\\\test.xml\", 'wb') as outfile:\n",
    "    outfile.write(response.read())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c49766a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SQL Server and MongoDB are both popular database management systems, but they differ in several key aspects. Here are \n",
    "# some major similarities and differences between the two:\n",
    "\n",
    "    \n",
    "\n",
    "# Similarities:\n",
    "#  Data Storage: Both SQL Server and MongoDB are used to store and manage data.\n",
    "#  Indexes: Both support indexing to improve query performance.\n",
    "#  Security: They offer security features such as user authentication, authorization, and encryption to protect data.\n",
    "#  Backup and Recovery: Both provide mechanisms for backup and recovery of data to ensure data integrity and availability.\n",
    "#  Scalability: Both SQL Server and MongoDB support scaling horizontally and vertically to handle increasing data volumes \n",
    "#  and user loads.\n",
    "\n",
    "\n",
    "\n",
    "# Differences:\n",
    "\n",
    "#  Data Model:\n",
    "#  SQL Server: Follows a relational data model and stores data in tables with rows and columns. It uses SQL \n",
    "#  (Structured Query Language) for querying data using predefined schemas.\n",
    "#  MongoDB: Follows a document-oriented data model and stores data in flexible, JSON-like documents. It does \n",
    "#  not require a predefined schema, allowing for dynamic and nested data structures. Queries are made using \n",
    "#  MongoDB's query language and operators.\n",
    "\n",
    "#  Schema:\n",
    "#  SQL Server: Requires a predefined schema where the structure of the data (tables, columns, data types, constraints) \n",
    "#  must be defined before data insertion.\n",
    "#  MongoDB: Allows flexible schema design where documents within a collection can have varying structures. It supports \n",
    "#  dynamic schema evolution, enabling easy modification of data structures without downtime.\n",
    "\n",
    "#  Query Language:\n",
    "#  SQL Server: Uses SQL (Structured Query Language) for querying data. SQL provides a standardized syntax for interacting \n",
    "#  with relational databases.\n",
    "#  MongoDB: Uses a query language similar to JavaScript and provides a rich set of operators for querying nested and \n",
    "#  complex data structures within documents.\n",
    "\n",
    "#  Transactions:\n",
    "#  SQL Server: Supports ACID (Atomicity, Consistency, Isolation, Durability) transactions for ensuring data integrity \n",
    "#  in multi-operation transactions.\n",
    "#  MongoDB: Supports atomic operations on a single document but does not support multi-document transactions across \n",
    "#  multiple collections/documents in a single transaction.\n",
    "\n",
    "#  Scaling:\n",
    "#  SQL Server: Typically scales vertically by adding more resources (CPU, memory, storage) to a single server.\n",
    "#  MongoDB: Scales horizontally by distributing data across multiple servers (sharding), allowing for linear scalability \n",
    "#  as data volume increases.\n",
    "\n",
    "#  Community and Ecosystem:\n",
    "#  SQL Server: Developed and maintained by Microsoft, with strong community support and a wide range of third-party tools \n",
    "#  and integrations.\n",
    "#  MongoDB: Developed and maintained by MongoDB Inc., with an active open-source community and a rich ecosystem of \n",
    "#  libraries, frameworks, and tools.\n",
    "\n",
    "#  Use Cases:\n",
    "#  SQL Server: Well-suited for applications with structured and relational data, such as transactional systems, enterprise \n",
    "#  applications, and data warehouses.\n",
    "#  MongoDB: Ideal for applications with unstructured or semi-structured data, real-time analytics, content management systems, \n",
    "#  and use cases requiring flexibility and scalability.\n",
    "\n",
    "# Overall, the choice between SQL Server and MongoDB depends on factors such as the nature of your data, the complexity of \n",
    "#  your application, scalability requirements, and development preferences.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
