{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  TensorFlow or PyTorch may outperform RandomForest Regressor in scenarios where the data has complex patterns or \n",
    "#  dependencies that are challenging for traditional machine learning models to capture. Here are some examples where \n",
    "#  deep learning models might be advantageous.\n",
    "\n",
    "#  Image Recognition:\n",
    "#  Deep learning models, especially convolutional neural networks (CNNs), excel in image recognition tasks. They can \n",
    "#  automatically learn hierarchical features from raw pixel values, capturing intricate patterns in images.\n",
    "\n",
    "#  Natural Language Processing (NLP):\n",
    "#  Recurrent Neural Networks (RNNs) or Transformer-based models (like BERT or GPT) are widely used for NLP tasks such as \n",
    "#  sentiment analysis, machine translation, and text generation. They can capture sequential dependencies and contextual \n",
    "#  information in text data.\n",
    "\n",
    "#  Speech Recognition:\n",
    "#  Deep learning models, particularly recurrent neural networks (RNNs) or deep neural networks (DNNs), are effective in \n",
    "#  speech recognition tasks. They can learn complex mappings from audio signals to textual representations.\n",
    "\n",
    "#  Time Series Forecasting:\n",
    "#  For time series data with long-term dependencies, recurrent neural networks (RNNs) or Long Short-Term Memory networks \n",
    "#  (LSTMs) may outperform traditional models. They can capture temporal patterns and dependencies over extended sequences.\n",
    "\n",
    "#  Unstructured Data:\n",
    "#  When dealing with unstructured data such as raw audio, video, or 3D data, deep learning models can automatically \n",
    "#  learn hierarchical representations without the need for handcrafted features.\n",
    "\n",
    "#  Transfer Learning:\n",
    "#  Pre-trained deep learning models, such as those trained on large image datasets (e.g., ImageNet), can be fine-tuned \n",
    "#  for specific tasks with limited labeled data. This is particularly beneficial when working with small datasets.\n",
    "\n",
    "#  Generative Tasks:\n",
    "#  Deep learning models, especially Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), are used \n",
    "#  for generative tasks such as image generation, style transfer, and data synthesis.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['mpg', 'cyl', 'disp', 'hp', 'drat', 'wt', 'qsec', 'vs', 'am', 'gear',\n",
      "       'carb'],\n",
      "      dtype='object')\n",
      "----------\n",
      "cyl       6.00\n",
      "disp    160.00\n",
      "hp      110.00\n",
      "drat      3.90\n",
      "wt        2.62\n",
      "qsec     16.46\n",
      "vs        0.00\n",
      "am        1.00\n",
      "gear      4.00\n",
      "carb      4.00\n",
      "Name: Mazda RX4, dtype: float64\n",
      "----------\n",
      "21.0\n",
      "----------\n",
      "Mean Absolute Error: 1.6839999999999964\n",
      "Predicted MPG: 20.82\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Here is a pretty simple scenario where a RandomForestRegressor, from the world of Scikit-Learn, outperforms more\n",
    "# complex methodologies TensorFlow and PyTorch.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import seaborn as sb\n",
    "import scipy\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn import datasets\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "mtcars = sm.datasets.get_rdataset(\"mtcars\", \"datasets\", cache=True).data\n",
    "df = pd.DataFrame(mtcars)\n",
    "print(df.columns)\n",
    "df.head()\n",
    "\n",
    "\n",
    "# Select features (X) and target variable (y)\n",
    "X = df[['cyl','disp','hp','drat','wt','qsec','vs','am','gear','carb']]\n",
    "y = df['mpg']\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# This line splits the data into training and testing sets using the train_test_split function. The training set \n",
    "# (80%) is used to train the model, and the testing set (20%) is used to evaluate its performance.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Create a RandomForestRegressor model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "\n",
    "# The model is trained on the training set using the fit method.\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# The trained model is used to make predictions on the testing set.\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print('----------')\n",
    "print(X.iloc[0])\n",
    "print('----------')\n",
    "print(y.iloc[0])\n",
    "print('----------')\n",
    "\n",
    "# The Mean Absolute Error (MAE) is calculated to evaluate the model's performance. MAE represents the average \n",
    "# absolute difference between the predicted and actual prices in the testing set.\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "\n",
    "\n",
    "new_data = pd.DataFrame({'cyl': [6], \n",
    "                         'disp': [160], \n",
    "                         'hp': [110], \n",
    "                         'drat': [3.90], \n",
    "                         'wt': [2.6], \n",
    "                         'qsec': [16.4], \n",
    "                         'vs': [0], \n",
    "                         'am': [1], \n",
    "                         'gear': [4], \n",
    "                         'carb': [4]})\n",
    "\n",
    "predicted_mpg = model.predict(new_data)\n",
    "print(f'Predicted MPG: {predicted_mpg[0]:,.2f}')\n",
    "\n",
    "# We know the actual MPG for the given independent variables is 21.0\n",
    "# we see that the predicted value is 20.82\n",
    "# The difference is: =21/20.82-1\n",
    "# 0.86%! Not bad!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ryan_\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ryan_\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ryan_\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From C:\\Users\\ryan_\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2146.9629 - val_loss: 2412.0081\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1924.2162 - val_loss: 2109.3716\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1715.5286 - val_loss: 1829.8031\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1521.8698 - val_loss: 1573.6959\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1342.3246 - val_loss: 1340.4518\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1176.8857 - val_loss: 1130.1862\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1025.7515 - val_loss: 944.0344\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 888.3498 - val_loss: 780.1611\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 763.9761 - val_loss: 638.0047\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 652.3417 - val_loss: 516.6807\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 552.9817 - val_loss: 414.8404\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 465.7823 - val_loss: 332.3429\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 390.2451 - val_loss: 268.1659\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 325.9279 - val_loss: 221.0668\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 272.2374 - val_loss: 189.8332\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 228.4333 - val_loss: 173.0963\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 193.6730 - val_loss: 169.1174\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 167.2015 - val_loss: 176.0963\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 148.1578 - val_loss: 192.1268\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 135.5565 - val_loss: 215.2651\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 128.3585 - val_loss: 243.7687\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 125.5796 - val_loss: 275.7233\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 126.2381 - val_loss: 308.7108\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 129.4005 - val_loss: 341.5197\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 134.2154 - val_loss: 372.7316\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 139.9026 - val_loss: 401.1266\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 145.7845 - val_loss: 425.8115\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 151.2403 - val_loss: 446.2882\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 155.8293 - val_loss: 461.8448\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 159.2949 - val_loss: 472.2639\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 161.4383 - val_loss: 477.5570\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 162.1933 - val_loss: 477.9302\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 161.5753 - val_loss: 473.7861\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 159.6787 - val_loss: 465.5938\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 156.6600 - val_loss: 453.9137\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 152.7197 - val_loss: 439.3688\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 148.0708 - val_loss: 422.5995\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 143.0259 - val_loss: 404.1838\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 137.7458 - val_loss: 384.7256\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 132.4047 - val_loss: 364.7794\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 127.1817 - val_loss: 344.8888\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 122.2199 - val_loss: 325.7462\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 117.6361 - val_loss: 307.4219\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 113.5111 - val_loss: 290.1545\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 109.8911 - val_loss: 274.0682\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 106.9042 - val_loss: 259.2552\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 104.5833 - val_loss: 245.9960\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 102.7357 - val_loss: 233.9818\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 101.2967 - val_loss: 223.2414\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 100.2123 - val_loss: 213.7586\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 195.2227\n",
      "Mean Squared Error: 195.22267150878906\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "Mean Absolute Error (TensorFlow): 10.331711769104004\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "Predicted MPG (TensorFlow): 15.94\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Using TensorFlow:\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define the model\n",
    "model_tf = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(10,)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_tf.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model_tf.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model_tf.evaluate(X_test, y_test)\n",
    "print(f'Mean Squared Error: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "predictions_tf = model_tf.predict(X_test)\n",
    "\n",
    "# Calculate Mean Absolute Error\n",
    "mae_tf = mean_absolute_error(y_test, predictions_tf)\n",
    "print(f'Mean Absolute Error (TensorFlow): {mae_tf}')\n",
    "\n",
    "# Make prediction on new data\n",
    "new_data_tf = np.array([[6, 160, 110, 3.90, 2.6, 16.4, 0, 1, 4, 4]])\n",
    "predicted_mpg_tf = model_tf.predict(new_data_tf)\n",
    "print(f'Predicted MPG (TensorFlow): {predicted_mpg_tf[0, 0]:,.2f}')\n",
    "\n",
    "# We know the actual MPG for the given independent variables is 21.0\n",
    "# we see that the predicted value is 15.94\n",
    "# The difference is: =21/15.94-1\n",
    "# 31.74%!!!! Not very accurate at all!!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (PyTorch): 163.92013549804688\n",
      "Mean Absolute Error (PyTorch): 10.22669335774013\n",
      "Predicted MPG (PyTorch): 10.62\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Using PyTorch:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_torch = torch.FloatTensor(X_train.values)\n",
    "y_train_torch = torch.FloatTensor(y_train.values).view(-1, 1)\n",
    "X_test_torch = torch.FloatTensor(X_test.values)\n",
    "y_test_torch = torch.FloatTensor(y_test.values).view(-1, 1)\n",
    "\n",
    "# Define the model\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model_pt = RegressionModel()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_pt.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model_pt(X_train_torch)\n",
    "    loss = criterion(outputs, y_train_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "with torch.no_grad():\n",
    "    outputs_test = model_pt(X_test_torch)\n",
    "    loss_pt = criterion(outputs_test, y_test_torch)\n",
    "    print(f'Mean Squared Error (PyTorch): {loss_pt.item()}')\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    predictions_pt = model_pt(X_test_torch).detach().numpy()\n",
    "\n",
    "\n",
    "# Calculate Mean Absolute Error\n",
    "mae_pt = mean_absolute_error(y_test, predictions_pt)\n",
    "print(f'Mean Absolute Error (PyTorch): {mae_pt}')\n",
    "\n",
    "\n",
    "# Make prediction on new data\n",
    "new_data_pt = torch.FloatTensor([[6, 160, 110, 3.90, 2.6, 16.4, 0, 1, 4, 4]])\n",
    "predicted_mpg_pt = model_pt(new_data_pt).item()\n",
    "print(f'Predicted MPG (PyTorch): {predicted_mpg_pt:,.2f}')\n",
    "\n",
    "# We know the actual MPG for the given independent variables is 21.0\n",
    "# we see that the predicted value is 15.94\n",
    "# The difference is: =21/10.62-1\n",
    "# 97.74%!!!! That's horrible!!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In the experiments shown above, the RandomForest Regressor performed significantly better in terms of MAE compared to \n",
    "# the deep learning models implemented with TensorFlow and PyTorch. A lower MAE indicates better predictive performance, \n",
    "# and in this case, the RandomForest Regressor achieved a more accurate prediction on the test set.\n",
    "\n",
    "# However, it's essential to consider the context and the complexity of the models. Deep learning models might require \n",
    "# more tuning, a larger dataset, or a more complex architecture to outperform traditional machine learning models like \n",
    "# RandomForest. In some cases, deep learning models shine when dealing with complex patterns and large datasets, while \n",
    "# simpler models might suffice for smaller or less complex datasets.\n",
    "\n",
    "# In practice, the choice of model depends on various factors such as dataset size, feature complexity, computational \n",
    "# resources, interpretability, and the specific requirements of the task at hand. Each model type has its strengths and \n",
    "# weaknesses, and the best choice often depends on the specific characteristics of the problem you are trying to solve.\n",
    "\n",
    "# In summary, simpler models like RandomForest Regressor may perform well, especially with smaller datasets or when \n",
    "# interpretability is crucial. The effectiveness of deep learning models often comes into play when dealing with large, \n",
    "# complex datasets with intricate patterns that are challenging for traditional models to capture.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
