{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# It's interesting to compare Causal Inference and Predictive Inference!\n",
    "\n",
    "\n",
    "#  Causal Inference:\n",
    "\n",
    "#  Objective:\n",
    "#  Causal inference aims to understand cause-and-effect relationships between variables.\n",
    "#  It seeks to answer questions like \"What is the effect of X on Y?\" or \"What causes Y to change when X changes?\"\n",
    "\n",
    "#  Methodology:\n",
    "#  Causal inference often relies on experimental design, observational studies, and statistical modeling to establish causality.\n",
    "\n",
    "#  Techniques like randomized controlled trials (RCTs), propensity score matching, instrumental variables, and structural \n",
    "#  equation modeling are commonly used.\n",
    "\n",
    "#  Assumptions:\n",
    "#  Causal inference usually requires strong assumptions about the data generation process, such as no unmeasured confounding \n",
    "#  variables and no selection bias.\n",
    "\n",
    "#  It often assumes temporal precedence, i.e., the cause must precede the effect in time.\n",
    "\n",
    "#  Interpretation:\n",
    "#  Causal inference provides insights into the underlying mechanisms driving observed relationships between variables.\n",
    "#  It helps identify interventions or policies that can change the outcome of interest.\n",
    "\n",
    "\n",
    "#  Predictive Inference:\n",
    "\n",
    "#  Objective:\n",
    "#  Predictive inference focuses on making accurate predictions about future or unseen data based on patterns observed in the \n",
    "#  training data.\n",
    "\n",
    "#  It aims to answer questions like \"What will be the value of Y given certain values of X?\"\n",
    "\n",
    "#  Methodology:\n",
    "#  Predictive inference utilizes machine learning algorithms and statistical models to learn patterns from historical data \n",
    "#  and generalize to new, unseen data points.\n",
    "\n",
    "#  Techniques like regression, classification, clustering, and deep learning are commonly used.\n",
    "\n",
    "#  Assumptions:\n",
    "#  Predictive inference assumes that the patterns observed in the training data will hold in the future or on unseen data.\n",
    "\n",
    "#  It does not necessarily require strong assumptions about the underlying data generation process or causal relationships.\n",
    "\n",
    "#  Interpretation:\n",
    "#  Predictive inference focuses on the accuracy of predictions rather than understanding the underlying causal mechanisms.\n",
    "\n",
    "#  It is often used for forecasting, risk assessment, decision-making, and optimization.\n",
    "\n",
    "\n",
    "#  Comparison:\n",
    "#  Focus: Causal inference focuses on understanding cause-and-effect relationships, while predictive inference focuses on \n",
    "#  making accurate predictions.\n",
    "\n",
    "#  Methodology: Both approaches rely on statistical modeling and machine learning techniques but differ in their objectives \n",
    "#  and assumptions.\n",
    "\n",
    "#  Interpretation: Causal inference provides insights into why certain phenomena occur, whereas predictive inference \n",
    "#  focuses on what will happen next based on historical data.\n",
    "\n",
    "#  Applications: Causal inference is commonly used in social sciences, economics, public health, and policy evaluation, \n",
    "#  while predictive inference finds applications in areas like finance, marketing, healthcare, and recommendation systems.\n",
    "        \n",
    "\n",
    "# let's run an experiment and see how things play out!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from causalnex.structure import StructureModel\n",
    "from causalnex.discretiser import Discretiser\n",
    "from causalnex.network import BayesianNetwork\n",
    "from causalnex.inference import InferenceEngine\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "\n",
    "# Generate synthetic data for crime prediction\n",
    "np.random.seed(42)\n",
    "\n",
    "# Features (independent variables)\n",
    "population_density = np.random.uniform(100, 1000, 1000)\n",
    "unemployment_rate = np.random.uniform(2, 15, 1000)\n",
    "poverty_rate = np.random.uniform(5, 30, 1000)\n",
    "\n",
    "# Continuous target variable (Crime rate)\n",
    "crime_rate = 50 + 2 * population_density + 3 * unemployment_rate + 5 * poverty_rate + np.random.normal(0, 10, 1000)\n",
    "\n",
    "# Create a DataFrame\n",
    "crime_data = pd.DataFrame({\n",
    "    'PopulationDensity': population_density,\n",
    "    'UnemploymentRate': unemployment_rate,\n",
    "    'PovertyRate': poverty_rate,\n",
    "    'CrimeRate': crime_rate\n",
    "})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryans\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\preprocessing\\_discretization.py:239: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ryans\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\preprocessing\\_discretization.py:239: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ryans\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\preprocessing\\_discretization.py:239: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Discretize the features using KMeansDiscretizer\n",
    "discretised_data = crime_data.copy()\n",
    "for feature in ['PopulationDensity', 'UnemploymentRate', 'PovertyRate']:\n",
    "    discretiser = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='kmeans')\n",
    "    discretised_data[feature] = discretiser.fit_transform(discretised_data[[feature]])\n",
    "    \n",
    "    \n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(discretised_data, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Define the structure of the Bayesian Network\n",
    "sm = StructureModel()\n",
    "sm.add_edges_from([\n",
    "    ('PopulationDensity', 'CrimeRate'),\n",
    "    ('UnemploymentRate', 'CrimeRate'),\n",
    "    ('PovertyRate', 'CrimeRate'),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryans\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\preprocessing\\_discretization.py:239: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Discretize the target variable\n",
    "# The double square brackets [['CrimeRate']] create a DataFrame with a single column, \n",
    "# ensuring that the input is a 2D array.\n",
    "train_data['CrimeRate'] = discretiser.fit_transform(train_data[['CrimeRate']])\n",
    "\n",
    "# Create and fit the Bayesian Network\n",
    "bn = BayesianNetwork(sm)\n",
    "bn.fit_node_states(train_data)\n",
    "bn.fit_cpds(train_data)\n",
    "\n",
    "\n",
    "# Use the Bayesian Network for causal inference\n",
    "ie = InferenceEngine(bn)\n",
    "causal_effects = ie.query()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal Effects on CrimeRate:\n",
      "PopulationDensity: {0.0: 0.22374999999999998, 1.0: 0.19875000000000004, 2.0: 0.18625, 3.0: 0.18000000000000005, 4.0: 0.21125}\n",
      "CrimeRate: {0.0: 0.22631938810239235, 1.0: 0.19308480017885765, 2.0: 0.19631787329799114, 3.0: 0.17803114766555056, 4.0: 0.2062467907552083}\n",
      "UnemploymentRate: {0.0: 0.1925, 1.0: 0.16374999999999995, 2.0: 0.195, 3.0: 0.2275, 4.0: 0.22125000000000003}\n",
      "PovertyRate: {0.0: 0.185, 1.0: 0.20750000000000002, 2.0: 0.19875000000000004, 3.0: 0.2, 4.0: 0.20875}\n",
      "Causal Effects on CrimeRate:\n",
      "PopulationDensity:\n",
      "  Effect Information: {0.0: 0.22374999999999998, 1.0: 0.19875000000000004, 2.0: 0.18625, 3.0: 0.18000000000000005, 4.0: 0.21125}\n",
      "\n",
      "CrimeRate:\n",
      "  Effect Information: {0.0: 0.22631938810239235, 1.0: 0.19308480017885765, 2.0: 0.19631787329799114, 3.0: 0.17803114766555056, 4.0: 0.2062467907552083}\n",
      "\n",
      "UnemploymentRate:\n",
      "  Effect Information: {0.0: 0.1925, 1.0: 0.16374999999999995, 2.0: 0.195, 3.0: 0.2275, 4.0: 0.22125000000000003}\n",
      "\n",
      "PovertyRate:\n",
      "  Effect Information: {0.0: 0.185, 1.0: 0.20750000000000002, 2.0: 0.19875000000000004, 3.0: 0.2, 4.0: 0.20875}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'Causal Effects on CrimeRate:')\n",
    "for variable, effect in causal_effects.items():\n",
    "    print(f'{variable}: {effect}')\n",
    "    \n",
    "\n",
    "# Print the interpretation of the results\n",
    "print(f'Causal Effects on CrimeRate:')\n",
    "for variable, effect in causal_effects.items():\n",
    "    print(f'{variable}:')\n",
    "    print(f'  Effect Information: {effect}')\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# How can we interpret the final results?\n",
    "\n",
    "# The estimated effects provided for each level (bin) of UnemploymentRate and PovertyRate represent the causal \n",
    "# effects of these variables on CrimeRate as predicted by the Bayesian Network model.\n",
    "\n",
    "# In causal inference, a causal effect is an estimate of the change in the outcome variable (in this case, \n",
    "# CrimeRate) that is directly attributable to a change in the predictor variable (such as UnemploymentRate or \n",
    "# PovertyRate). \n",
    "\n",
    "\n",
    "# UnemploymentRate:\n",
    "\n",
    "#  For each bin of UnemploymentRate (0.0, 1.0, 2.0, 3.0, 4.0), the estimated effect on CrimeRate is given.\n",
    "#  For example, when UnemploymentRate is in the bin 0.0, the estimated effect on CrimeRate is approximately \n",
    "#  0.1925. Similarly, for other bins.\n",
    "\n",
    "\n",
    "# PovertyRate:\n",
    "\n",
    "#  Similar interpretation applies to PovertyRate. Each bin of PovertyRate (0.0, 1.0, 2.0, 3.0, 4.0) has an \n",
    "#  estimated effect on CrimeRate.\n",
    "#  For example, when PovertyRate is in the bin 0.0, the estimated effect on CrimeRate is approximately 0.1850. \n",
    "#  Similarly, for other bins.\n",
    "\n",
    "\n",
    "# These values represent the estimated change in CrimeRate associated with a one-unit increase in the respective \n",
    "# bin of UnemploymentRate or PovertyRate. The positive values suggest a positive association, indicating that \n",
    "# higher levels of UnemploymentRate or PovertyRate are associated with higher CrimeRate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 444.99555596977035\n",
      "R^2 Score: 0.9983832782727251\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# To perform predictive inference using the same data source, you can train a machine learning model to predict the \n",
    "# target variable based on the features. Below is the modified code that utilizes a Random Forest regressor for \n",
    "# predictive inference.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate synthetic data for crime prediction\n",
    "np.random.seed(42)\n",
    "\n",
    "# Features (independent variables)\n",
    "population_density = np.random.uniform(100, 1000, 1000)\n",
    "unemployment_rate = np.random.uniform(2, 15, 1000)\n",
    "poverty_rate = np.random.uniform(5, 30, 1000)\n",
    "\n",
    "# Continuous target variable (Crime rate)\n",
    "crime_rate = 50 + 2 * population_density + 3 * unemployment_rate + 5 * poverty_rate + np.random.normal(0, 10, 1000)\n",
    "\n",
    "# Create a DataFrame\n",
    "crime_data = pd.DataFrame({\n",
    "    'PopulationDensity': population_density,\n",
    "    'UnemploymentRate': unemployment_rate,\n",
    "    'PovertyRate': poverty_rate,\n",
    "    'CrimeRate': crime_rate\n",
    "})\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = crime_data.drop(columns=['CrimeRate'])\n",
    "y = crime_data['CrimeRate']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest regressor\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_regressor.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error as evaluation metric\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Calculate R^2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'R^2 Score: {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  As an aside Feature importance is a concept within machine learning that quantifies the contribution of each feature \n",
    "#  (independent variable) to the predictive performance of a model. It helps identify which features are most influential \n",
    "#  in making predictions, providing insights into the relative importance of different variables. Feature importance is\n",
    "#  generally derived from the model's internal mechanisms, such as coefficients in linear models or impurity reduction \n",
    "#  in tree-based models.\n",
    "\n",
    "#  You can learn about Feature Importance here:\n",
    "#  https://github.com/ash-wicus-ml/Notebooks/blob/master/XG%20Boost%20-%20Feature%20Importance.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
