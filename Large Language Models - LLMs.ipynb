{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Below, we are comparing and contrasting two types of Large Language Models (LLMs):\n",
    "#  GPT2LMHeadModel\n",
    "#  BertForTokenClassification\n",
    "\n",
    "#  GPT2LMHeadModel:\n",
    "\n",
    "#  Purpose:\n",
    "#  GPT2LMHeadModel is primarily used for autoregressive language modeling tasks, such as text generation, \n",
    "#  text completion, and next-token prediction.\n",
    "#  It predicts the likelihood of the next token in a sequence given the preceding tokens.\n",
    "\n",
    "#  Architecture:\n",
    "#  GPT2LMHeadModel is based on the Transformer architecture, specifically the decoder part of the Transformer.\n",
    "#  It consists of multiple layers of self-attention mechanisms and feed-forward neural networks.\n",
    "\n",
    "#  Output:\n",
    "#  The output of GPT2LMHeadModel is a probability distribution over the vocabulary, indicating the \n",
    "#  likelihood of each token being the next token in the sequence.\n",
    "\n",
    "#  Example Applications:\n",
    "#  Text generation\n",
    "#  Dialog systems\n",
    "#  Language translation (in combination with an encoder-decoder architecture)\n",
    "\n",
    "\n",
    "#  BertForTokenClassification:\n",
    "\n",
    "#  Purpose:\n",
    "#  BertForTokenClassification is designed for token-level classification tasks, such as Named Entity \n",
    "#  Recognition (NER), part-of-speech tagging, and semantic role labeling.\n",
    "#  It assigns a label to each token in the input sequence.\n",
    "\n",
    "#  Architecture:\n",
    "#  BertForTokenClassification is based on the Transformer architecture, specifically the encoder part \n",
    "#  of the Transformer.\n",
    "#  It uses a token-level classification head on top of the pre-trained BERT encoder.\n",
    "\n",
    "#  Output:\n",
    "#  The output of BertForTokenClassification is a sequence of logits, one for each token in the input \n",
    "#  sequence, indicating the probability of each token belonging to each class.\n",
    "\n",
    "#  Example Applications:\n",
    "#  Named Entity Recognition (NER)\n",
    "#  Part-of-speech tagging\n",
    "#  Semantic role labeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GPT2LMHeadModel stands for \"Generative Pre-trained Transformer 2 Language Model with a Language \n",
    "# Modeling Head.\"\n",
    "\n",
    "# Generative: This refers to the ability of the model to generate new text based on a given prompt or context.\n",
    " \n",
    "# Pre-trained: The model is pre-trained on a large corpus of text data before being fine-tuned on a \n",
    "# specific task. \n",
    "\n",
    "# Transformer: Transformers are a type of neural network architecture that relies on self-attention \n",
    "# mechanisms to capture long-range dependencies in sequences more effectively than traditional recurrent \n",
    "# neural networks (RNNs) or convolutional neural networks (CNNs).\n",
    "\n",
    "# 2: GPT-2 is the second version of the Generative Pre-trained Transformer model developed by OpenAI.\n",
    "\n",
    "# Language Model: GPT-2 is primarily a language model, which means it is trained to predict the \n",
    "# likelihood of a word or token occurring in a sequence given the context of the preceding words. \n",
    "\n",
    "# LMHeadModel: the LMHeadModel consists of a linear transformation followed by a softmax activation \n",
    "# function, which generates probability distributions over the vocabulary space to predict the next \n",
    "# token in a sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize Faker to generate fake data\n",
    "faker = Faker()\n",
    "\n",
    "# Define the number of samples to generate\n",
    "num_samples = 1000\n",
    "\n",
    "# Generate synthetic data for text generation, summarization, and sentiment analysis\n",
    "text_data = [faker.text(max_nb_chars=random.randint(100, 500)) for _ in range(num_samples)]\n",
    "summary_data = [faker.sentence() for _ in range(num_samples)]\n",
    "sentiment_data = [random.choice(['positive', 'negative', 'neutral']) for _ in range(num_samples)]\n",
    "\n",
    "# Save synthetic data to files\n",
    "with open('C:\\\\Users\\\\ryan_\\\\text_generation_data.txt', 'w') as file:\n",
    "    file.write('\\n'.join(text_data))\n",
    "\n",
    "with open('C:\\\\Users\\\\ryan_\\\\summarization_data.txt', 'w') as file:\n",
    "    file.write('\\n'.join(summary_data))\n",
    "\n",
    "with open('C:\\\\Users\\\\ryan_\\\\sentiment_analysis_data.txt', 'w') as file:\n",
    "    file.write('\\n'.join(sentiment_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='190' max='190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [190/190 36:28, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'I am hungry when.\\nSo far you.\\nDoctor today will.\\nWear today my.\\nValue she we.\\nStand away us your.\\nSeal rate us.\\nTreat someone tonight.\\nFight off for choice.'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Define training dataset\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"C:\\\\Users\\\\ryan_\\\\text_generation_data.txt\",  # Path to your training data file\n",
    "    block_size=128  # Adjust according to your dataset and memory constraints\n",
    ")\n",
    "\n",
    "# Define data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"C:\\\\Users\\\\ryan_\\\\\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,  # Number of training epochs\n",
    "    per_device_train_batch_size=4,  # Batch size per GPU/CPU\n",
    "    save_steps=2000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    logging_dir=\"C:\\\\Users\\\\ryan_\\\\\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"C:\\\\Users\\\\ryan_\\\\gpt2-finetuned\")\n",
    "tokenizer.save_pretrained(\"C:\\\\Users\\\\ryan_\\\\gpt2-finetuned\")\n",
    "\n",
    "\n",
    "# Finally...\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model\n",
    "fine_tuned_model = GPT2LMHeadModel.from_pretrained(\"C:\\\\Users\\\\ryan_\\\\gpt2-finetuned\")\n",
    "\n",
    "# Create a text generation pipeline\n",
    "text_generator = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "generated_text = text_generator(\"I am hungry\", max_length=50, num_return_sequences=1)\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# [{'generated_text': 'I am hungry in whom. Bring along of whom.\\nWorry worker thus increase land. Build \n",
    "# away throughout purpose.\\nKnow evidence upon.\\nDoctor claim action throughout. Hear analysis recognize \n",
    "# surface. Eat surface truth give.\\nDoctor analysis prove.'}]\n",
    "\n",
    "\n",
    "# This output is what the fine-tuned GPT-2 model generated based on the prompt \"I am hungry\" and \n",
    "# its own learned patterns from the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Result:\n",
    "\n",
    "# The prompt was \"I am hungry\", and the model generated a sequence of text following that prompt.\n",
    "\n",
    "# Explanation:\n",
    "# The text generation process involves the model predicting the next word or token based on the input prompt and the \n",
    "# context learned during training.\n",
    "\n",
    "# The model generates text sequentially, word by word, based on the probability distribution of the next token given the \n",
    "# preceding context.\n",
    "\n",
    "# The output is a single sequence of generated text.\n",
    "\n",
    "# The number of sequences returned (num_return_sequences) was set to 1, so there is one generated sequence.\n",
    "\n",
    "# The maximum length of the generated text (max_length) was set to 50 tokens, limiting the length of the generated text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based language model \n",
    "# architecture. It consists of multiple layers of self-attention and feed-forward neural networks. \n",
    "# BERT utilizes a bidirectional approach to capture contextual information from preceding and following \n",
    "# words in a sentence.\n",
    "\n",
    "# ForSequenceClassification: Sequence classification refers to tasks where the goal is to classify an \n",
    "# entire sequence of tokens (such as a sentence or paragraph) into one or more predefined categories or \n",
    "# labels. Examples of sequence classification tasks include sentiment analysis, text categorization, natural \n",
    "# language inference, and more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\ryan_\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8629999756813049\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Preprocess the data\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\ryan_\\\\Desktop\\\\sentiment_analysis.csv\", encoding='latin-1')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Convert df.text.values to a list of strings\n",
    "texts = [str(text) for text in df.text.values]\n",
    "\n",
    "# Encode the data\n",
    "encoded_data = tokenizer.batch_encode_plus(texts, add_special_tokens=True, return_attention_mask=True, pad_to_max_length=True, max_length=256, return_tensors='pt')\n",
    "\n",
    "input_ids = encoded_data['input_ids']\n",
    "attention_masks = encoded_data['attention_mask']\n",
    "\n",
    "\n",
    "# Initialize label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode string labels to numerical labels\n",
    "df['encoded_sentiment'] = label_encoder.fit_transform(df['sentiment'])\n",
    "\n",
    "# Convert numerical labels to tensor\n",
    "labels = torch.tensor(df['encoded_sentiment'].values)\n",
    "\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "# num_labels=3: positive, neutral, negative\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3, output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "# Define the training parameters\n",
    "batch_size = 24\n",
    "epochs = 3\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i in range(0, input_ids.size(0), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids[i:i+batch_size], attention_mask=attention_masks[i:i+batch_size], labels=labels[i:i+batch_size].long())  # Convert labels to torch.long\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_masks)\n",
    "    predictions = torch.argmax(outputs[0], dim=1).flatten()\n",
    "    accuracy = torch.sum(predictions == labels) / len(labels)\n",
    "\n",
    "print(\"Accuracy:\", accuracy.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocess new text data\n",
    "new_text = \"This is a great product! I love it.\"\n",
    "encoded_new_data = tokenizer(new_text, return_tensors='pt')\n",
    "\n",
    "# Pass data through the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_new_data)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Post-process predictions\n",
    "predicted_class = torch.argmax(logits, dim=1).item()\n",
    "predicted_sentiment = label_encoder.inverse_transform([predicted_class])[0]\n",
    "\n",
    "print(\"Predicted Sentiment:\", predicted_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ryan_\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75cd94884724e9daf45cdc2746ac491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryan_\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ryan_\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Once upon a time, I became very comfortable as a person of nature. I can see what's happening in my body, on the ice. When I was in hospital and I was diagnosed with cancer, the first treatments were to try to kill you\"}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load pre-trained GPT model and tokenizer\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Generate text\n",
    "generated_text = generator(\"Once upon a time\")\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Result:\n",
    "\n",
    "# The result is a generated text based on the input prompt \"Once upon a time\" using a pre-trained GPT (Generative Pre-trained \n",
    "# Transformer) model.\n",
    "\n",
    "# Interpretation:\n",
    "\n",
    "# The generated text starts with the common phrase \"Once upon a time,\" which often indicates the beginning of a story.\n",
    "\n",
    "# The continuation of the text appears to describe a personal experience or reflection. It mentions becoming comfortable as \n",
    "# a person of nature and observing one's body.\n",
    "\n",
    "# There is a sudden shift in the narrative when it mentions being in the hospital and being diagnosed with cancer. The text \n",
    "# describes the initial treatments aimed at addressing the diagnosis.\n",
    "\n",
    "# Context:\n",
    "# The context and coherence of the generated text may seem fragmented or nonsensical. This is because the GPT model generates \n",
    "# text based on patterns it learned from a large corpus of text data. While it can produce coherent and contextually \n",
    "# relevant text, it may also produce unexpected or unrelated sequences, especially when prompted with short or ambiguous input.\n",
    "\n",
    "# Evaluation:\n",
    "# The quality of generated text can vary based on factors such as the specific prompt provided, the complexity of the \n",
    "# language model, and the intended use case.\n",
    "\n",
    "# In this case, the generated text appears to include elements of storytelling and personal reflection, but it may not \n",
    "# necessarily align with coherent storytelling or factual accuracy.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Similarities between GPT2LMHeadModel and BertForTokenClassification:\n",
    "#  Both based on Transformer: Both GPT2LMHeadModel and BertForTokenClassification are based on the \n",
    "#  Transformer architecture, which is known for its effectiveness in modeling sequential data.\n",
    "\n",
    "#  Pre-trained Models: Both models are typically pre-trained on large corpora of text data using \n",
    "#  self-supervised learning objectives before being fine-tuned on downstream tasks.\n",
    "\n",
    "#  Hugging Face Transformers Library: Both models are part of the Transformers library developed by \n",
    "#  Hugging Face, providing a consistent interface for working with a wide range of transformer-based models.\n",
    "\n",
    "#  Differences between GPT2LMHeadModel and BertForTokenClassification:\n",
    "#  Task: The primary difference between the two models is the task they are designed for. GPT2LMHeadModel \n",
    "#  is used for autoregressive language modeling, while BertForTokenClassification is used for token-level \n",
    "#  classification tasks.\n",
    "\n",
    "#  Output: GPT2LMHeadModel outputs a probability distribution over the vocabulary for generating the next \n",
    "#  token, whereas BertForTokenClassification outputs a sequence of logits for classifying each token.\n",
    "\n",
    "#  Architecture Focus: While both models are based on the Transformer architecture, GPT2LMHeadModel \n",
    "#  focuses on the decoder part of the Transformer, whereas BertForTokenClassification focuses on the \n",
    "#  encoder part.\n",
    "\n",
    "#  In summary, GPT2LMHeadModel and BertForTokenClassification are both powerful models based on the \n",
    "#  Transformer architecture, but they are designed for different tasks and have different output formats. \n",
    "#  GPT2LMHeadModel is used for autoregressive language modeling, while BertForTokenClassification is \n",
    "#  used for token-level classification tasks such as Named Entity Recognition (NER). NEW is a natural \n",
    "#  language processing (NLP) task that involves identifying and classifying named entities in text into \n",
    "#  predefined categories such as the names of persons, organizations, locations, dates, quantities, \n",
    "#  monetary values, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOTICE: Both GPT2LMHeadModel and BertForTokenClassification can be relatively slow during inference, \n",
    "# especially when using large models or processing long sequences of text. This is because these models \n",
    "# are computationally intensive and require significant computational resources to generate predictions \n",
    "# or classify tokens accurately.\n",
    "\n",
    "# To improve the speed of inference or training, these models can be run on specialized hardware \n",
    "# accelerators such as GPUs (Graphics Processing Units) or TPUs (Tensor Processing Units). GPUs and TPUs \n",
    "# are optimized for parallel processing tasks like deep learning, allowing them to perform computations \n",
    "# much faster than traditional CPUs (Central Processing Units).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# END...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
